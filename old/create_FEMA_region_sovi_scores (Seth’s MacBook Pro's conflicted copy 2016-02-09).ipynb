{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The Validity of the SoVi Index\n",
    "###Seth E.  Spielman, David Folch, Joseph Tuccillo\n",
    "\n",
    "This is a script to calculate the Social Vulnerability (Sovi) Index at multiple geographic scales and usign different sets of input variables.  The Sovi index assess a places social vulnerability to natural hazards and has been used in hundreds of publications, in both the academic and policy domains.  We show that this index fails to meet certain intuitive assumptions about the nature of measurement of social vulnerability.\n",
    "\n",
    "The aim is to illustrate instability in the index and problems with *convergent validity*.  Convergent Validity is the idea that multiple measurements of the same thing, using a valid instruments, should yield similar *absolute* and *relative* measurements.  For example two thermometers measuing the same cup of water should yield the same approximate temperature- this would be an example of validitiy of absolute measurement.  An less rigorous concept of validity is to consider three cups ordered from hottest (_**A**_) to coldest (_**C**_), the two thermometers would be valid in a *relative* sense if their measurements of cups A, B, C differed absolutely (they measured different temperatures for each cup) but still placed cup _**A**_ as the warmest and cup _**C**_ as the coldest.\n",
    "\n",
    "We will show in this analysis that the Sovi Index fails this \"cup test\" that is it often mixes up the orders of the cup.  Counties in the United States that are high vulnerability at one scale of measurement (or form the index) are often low vulnerability in a slightly different version of the index.\n",
    "\n",
    "##Variables and Components\n",
    "The Sovi Index is constructed using a tecnhnique called Principal Components Analysis, this is matrix decomposition method that uses the covariance matrix of the input data.  Usually, in the social sciences one treats the \"compents\" what come of out a PCA as latent variables.  For example, in Sovi it comon to fine components that measure things like \"race and class\".  In this analysis we also show that this latent variable approach has maked some underlying problems with the Soci index, namely that variables contribute to the index in ways that are profoundly counter intuitive.  \n",
    "\n",
    "##There is a paper\n",
    "For an in-depth discussion of these ideas please see the companion paper to this anlysis [URL]() or contact the suthors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data Prep\n",
    "\n",
    "In this section we read in data from the American Community Survey and the Decennial Census and processes the varaibles such that they correspond to the inputs used to commonly construct the Sovi Index,  There is **a lot of data wrangling here**, combining variables into new variables, computing standard errors, etc.  Its all farily straightforward and I will not talk through the details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named mdp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-66ee24bec82f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrankdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspearmanr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspss_pca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSPSS_PCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlocal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/Seth/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Seth/Desktop/SoVI_var_wise_paper/code/spss_pca.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzscore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mZSCORE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmdp\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMDP\u001b[0m  \u001b[0;31m# causing sklearn deprication warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named mdp"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pysal as ps\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.mstats import zscore as ZSCORE\n",
    "from scipy.stats import rankdata\n",
    "from scipy.stats import spearmanr \n",
    "from spss_pca import SPSS_PCA\n",
    "\n",
    "local_path = '/Users/Seth/'\n",
    "#local_path = '/Users/dfolch/'\n",
    "\n",
    "os.chdir(local_path+'Dropbox/SoVI_var_wise_paper/code')\n",
    "path=local_path+'/Dropbox/SoVI_var_wise_paper'\n",
    "outPath =local_path+'/Dropbox/SoVI_var_wise_paper/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Functions for calculating Standard Errors\n",
    "\n",
    "THe functions below are to calculate the standard error (SE) of various types of estimates from the American Cummunity Survey.  These are provided by the US Census Bureau in the ACS Techncial Documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SE of a sum\n",
    "def se_sum(*ses):\n",
    "    df_temp = pd.DataFrame(list(ses))\n",
    "    df_temp = df_temp.T\n",
    "    df_temp = np.square(df_temp)\n",
    "    df_temp = df_temp.sum(1)\n",
    "    return np.sqrt(df_temp)\n",
    "\n",
    "#SE of a ratio\n",
    "def se_ratio(est, estd, sen, sed):\n",
    "    sen2 = np.square(sen)\n",
    "    sed2 = np.square(sed)\n",
    "    est2 = np.square(est)\n",
    "    num = np.sqrt(sen2 + (est2*sed2))\n",
    "    return num / estd\n",
    "\n",
    "#SE of a proprotion\n",
    "def se_prop(est, estd, sen, sed):\n",
    "    sen2 = np.square(sen)\n",
    "    sed2 = np.square(sed)\n",
    "    est2 = np.square(est)\n",
    "    num = sen2 - (est2*sed2)\n",
    "    num_alt = sen2 + (est2*sed2)\n",
    "    problems = num <= 0\n",
    "    num[problems] = num_alt[problems]\n",
    "    num = np.sqrt(num)\n",
    "    return num / estd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%capture capt\n",
    "pd.set_option(\"chained_assignment\", None)\n",
    "\n",
    "path=local_path+\"Dropbox/SoVI_var_wise_paper/data/input\"\n",
    "spath=local_path+\"Dropbox/SoVI_var_wise_paper/data/spatial\"\n",
    "\n",
    "make_strings = {'Geo_FIPS':object, 'Geo_STATE':object, 'Geo_COUNTY':object, \n",
    "                'Geo_TRACT':object, 'Geo_CBSA':object, 'Geo_CSA':object}\n",
    "acs = pd.read_csv(os.path.join(path,'sovi_acs.csv'), dtype=make_strings, skiprows=1)\n",
    "acs.index = 'g' + acs.Geo_FIPS\n",
    "census = pd.read_csv(os.path.join(path,'sovi_decennial.csv'), dtype=make_strings, skiprows=1)\n",
    "census.index = 'g' + census.Geo_FIPS\n",
    "db = census\n",
    "db = db.join(acs, rsuffix='_acs')\n",
    "\n",
    "acs_samp = pd.read_csv(os.path.join(path,'sovi_acs_sampSize.csv'), dtype=make_strings, skiprows=1)\n",
    "acs_samp.index = 'g' + acs_samp.Geo_FIPS\n",
    "db = db.join(acs_samp, rsuffix='_acsSamp')\n",
    "\n",
    "try:\n",
    "    census_sup1 = pd.read_csv(os.path.join(path,'sovi_decennial_sup1.csv'), dtype=make_strings, skiprows=1)\n",
    "    census_sup1.index = 'g' + census_sup1.Geo_FIPS\n",
    "    db = db.join(census_sup1, rsuffix='_decSup1')\n",
    "except:\n",
    "    print 'no supplementary decennial data'\n",
    "try:\n",
    "    acs_sup1 = pd.read_csv(os.path.join(spath,'sovi_acs_sup1.csv'), dtype=make_strings, skiprows=1)\n",
    "    acs_sup1.index = 'g' + acs_sup1.Geo_FIPS\n",
    "    db = db.join(acs_sup1, rsuffix='_acsSup1')\n",
    "except:\n",
    "    print 'did not pull supplementary ACS data - A'\n",
    "try:\n",
    "    acs_sup2 = pd.read_csv(os.path.join(path,'sovi_acs_kids.csv'), dtype=make_strings, skiprows=1)\n",
    "    acs_sup2.index = 'g' + acs_sup2.Geo_FIPS\n",
    "    db = db.join(acs_sup2, rsuffix='_acsSup2')\n",
    "except:\n",
    "    print 'did not pull supplementary ACS data - B'\n",
    "    \n",
    "# drop Puerto Rico (sorry PR!)\n",
    "db = db[db.Geo_STATE != '72']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Preparing Sovi Inputs and Calculating SE\n",
    "\n",
    "A few counties are missing data so we create imput the values for these counties by taking the average value of their neighbors.  This is done using the spatial weights matrix `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%capture capt\n",
    "\n",
    "# cleanup Social Explorer standard errors\n",
    "se_cols = [i for i in db.columns if i[-1]=='s' and i[0]=='A']\n",
    "db[se_cols] *= (1.65/1.645)\n",
    "\n",
    "# weights matrix for computing spatial lags of missing data\n",
    "w = ps.queen_from_shapefile(os.path.join(spath,'USA_Counties_500k.shp'), idVariable='geoFIPS')\n",
    "w.transform = 'R'\n",
    "\n",
    "\n",
    "# output dataframe\n",
    "db1 = pd.DataFrame(index=db.index)\n",
    "\n",
    "# Decennial variables (original)\n",
    "db1['MEDAGE']  =  db.SF1_P0130001\n",
    "db1['BLACK']   = (db.SF1_P0030003 * 1.) / db.SF1_P0010001\n",
    "db1['QNATAM']  = (db.SF1_P0030004 * 1.) / db.SF1_P0010001\n",
    "db1['QASIAN']  = (db.SF1_P0030005 * 1.) / db.SF1_P0010001\n",
    "db1['QHISP']   = (db.SF1_P0040003 * 1.) / db.SF1_P0010001\n",
    "db1['QAGEDEP'] =((db.SF1_P0120003 + db.SF1_P0120027 + db.SF1_P0120020 + \n",
    "                  db.SF1_P0120021 + db.SF1_P0120022 + db.SF1_P0120023 + \n",
    "                  db.SF1_P0120024 + db.SF1_P0120025 + db.SF1_P0120044 + \n",
    "                  db.SF1_P0120045 + db.SF1_P0120046 + db.SF1_P0120047 + \n",
    "                  db.SF1_P0120048 + db.SF1_P0120049) * 1.) / db.SF1_P0010001\n",
    "db1['PPUNIT']  =  db.SF1_H0100001 / (db.SF1_H0030002 * 1.)\n",
    "db1['PRENTER'] = (db.SF1_H0040004 * 1.) / db.SF1_H0010001\n",
    "db1['QNRRES']  = (db.SF1_P0420005 * 1.) / db.SF1_P0010001\n",
    "db1['QFEMALE'] = (db.SF1_P0120026 * 1.) / db.SF1_P0010001\n",
    "db1['QFHH']    = (db.SF1_P0190014 * 1.) / db.SF1_P0180001\n",
    "db1['QUNOCCHU']=((db.SF1_H0010001 - db.SF1_H0030002) * 1.) / db.SF1_H0010001\n",
    "\n",
    "# Decennial variables (alternatives)\n",
    "db1['BLACK_ALT']   = (db.SF1_P0050004 * 1.) / db.SF1_P0010001 # exclude hispanic\n",
    "db1['QNATAM_ALT']  = (db.SF1_P0050005 * 1.) / db.SF1_P0010001 # exclude hispanic\n",
    "db1['QASIAN_ALT']  = (db.SF1_P0050006 * 1.) / db.SF1_P0010001 # exclude hispanic\n",
    "db1['QNRRES_ALT']  = (db.SF1_P0430023 + db.SF1_P0430054 * 1.) / db.SF1_P0010001 # 65 and over living in group quarters\n",
    "db1['QUNOCCHU_ALT']= (db.SF1_H0030003 * 1.) / db.SF1_H0030001  # same value, simplified computation\n",
    "\n",
    "# Decennial variables (using ACS data and alternative formulations)\n",
    "db1['MEDAGE_ACS']  = db.ACS12_5yr_B01002001\n",
    "db1['BLACK_ACS']   = db.ACS12_5yr_B03002004 / (db.ACS12_5yr_B03002001 * 1.)\n",
    "db1['QNATAM_ACS']  = db.ACS12_5yr_B03002005 / (db.ACS12_5yr_B03002001 * 1.)\n",
    "db1['QASIAN_ACS']  = db.ACS12_5yr_B03002006 / (db.ACS12_5yr_B03002001 * 1.)\n",
    "db1['QHISP_ACS']   = db.ACS12_5yr_B03002012 / (db.ACS12_5yr_B03002001 * 1.)\n",
    "db1['QAGEDEP_ACS'] =(db.ACS12_5yr_B06001002 + db.ACS12_5yr_B09020001) / (db.ACS12_5yr_B01003001 * 1.)\n",
    "db1['QPUNIT_ACS']  = db.ACS12_5yr_B25008001 / (db.ACS12_5yr_B25002002 * 1.)\n",
    "db1['PRENTER_ACS'] = db.ACS12_5yr_B25003003 / (db.ACS12_5yr_B25002001 * 1.)\n",
    "db1['QNRRES_ACS']  = db.ACS12_5yr_B09020021 / (db.ACS12_5yr_B01003001 * 1.)\n",
    "db1['QFEMALE_ACS'] = db.ACS12_5yr_B01001026 / (db.ACS12_5yr_B01003001 * 1.) \n",
    "db1['QFHH_ACS']    = db.ACS12_5yr_B11001006 / (db.ACS12_5yr_B11001001 * 1.)\n",
    "db1['QUNOCCHU_ACS']= db.ACS12_5yr_B25002003 / (db.ACS12_5yr_B25002001 * 1.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "\n",
    "# ACS variables (original)\n",
    "db1['PERCAP']   =  db.ACS12_5yr_B19025001 / (db.ACS12_5yr_B01003001 * 1.)\n",
    "db1['QESL']     =((db.ACS12_5yr_B16004029 + db.ACS12_5yr_B16004030 +\n",
    "                   db.ACS12_5yr_B16004034 + db.ACS12_5yr_B16004035 +\n",
    "                   db.ACS12_5yr_B16004039 + db.ACS12_5yr_B16004040 +\n",
    "                   db.ACS12_5yr_B16004044 + db.ACS12_5yr_B16004045 +\n",
    "                   db.ACS12_5yr_B16004051 + db.ACS12_5yr_B16004052 +\n",
    "                   db.ACS12_5yr_B16004056 + db.ACS12_5yr_B16004057 +\n",
    "                   db.ACS12_5yr_B16004061 + db.ACS12_5yr_B16004062 +\n",
    "                   db.ACS12_5yr_B16004066 + db.ACS12_5yr_B16004067) * 1.) /\\\n",
    "                 ((db.ACS12_5yr_B16004024 + db.ACS12_5yr_B16004046) - \n",
    "                  (db.ACS12_5yr_B16004025 + db.ACS12_5yr_B16004047))\n",
    "db1.QESL = db1.QESL.replace([np.inf,-np.inf, np.nan], 0)\n",
    "db1.QESL = db1.QESL.replace([np.inf,-np.inf], 0)\n",
    "db1['QCVLUN']   =((db.ACS12_5yr_B23022025 + db.ACS12_5yr_B23022049) * 1.) /\\\n",
    "                   db.ACS12_5yr_B23022001\n",
    "db1['QPOVTY']   = (db.ACS12_5yr_B17021002 * 1.) / db.ACS12_5yr_B17021001\n",
    "db1['QMOHO']    = (db.ACS12_5yr_B25024010 * 1.) / db.ACS12_5yr_B25024001\n",
    "db1['QED12LES'] =((db.ACS12_5yr_B15002003 + db.ACS12_5yr_B15002004 +\n",
    "                   db.ACS12_5yr_B15002005 + db.ACS12_5yr_B15002006 +\n",
    "                   db.ACS12_5yr_B15002007 + db.ACS12_5yr_B15002008 +\n",
    "                   db.ACS12_5yr_B15002009 + db.ACS12_5yr_B15002010 +\n",
    "                   db.ACS12_5yr_B15002020 + db.ACS12_5yr_B15002021 +\n",
    "                   db.ACS12_5yr_B15002022 + db.ACS12_5yr_B15002023 +\n",
    "                   db.ACS12_5yr_B15002024 + db.ACS12_5yr_B15002025 +\n",
    "                   db.ACS12_5yr_B15002026 + db.ACS12_5yr_B15002027) * 1.) /\\\n",
    "                   db.ACS12_5yr_B15002001\n",
    "db1['QFEMLBR']  = (db.ACS12_5yr_C24010038 * 1.) / db.ACS12_5yr_C24010001\n",
    "db1['QEXTRCT']  =((db.ACS12_5yr_C24030003 + db.ACS12_5yr_C24030030) * 1.) /\\\n",
    "                   db.ACS12_5yr_C24030001\n",
    "db1['QSERV']    =((db.ACS12_5yr_C24010019 + db.ACS12_5yr_C24010055) * 1.) /\\\n",
    "                   db.ACS12_5yr_C24010001\n",
    "db1['QSSBEN']   = (db.ACS12_5yr_B19055002 * 1.) / db.ACS12_5yr_B19055001\n",
    "db1['QNOAUTO']  =((db.ACS12_5yr_B25044003 + db.ACS12_5yr_B25044010) * 1.) /\\\n",
    "                   db.ACS12_5yr_B25044001\n",
    "db1['QFAM']     = (db.ACS12_5yr_B09002002 * 1.) / db.ACS12_5yr_B09002001\n",
    "db1.QFAM = db1.QFAM.replace([np.inf,-np.inf, np.nan], 0)\n",
    "db1['QRICH200K']= (db.ACS12_5yr_B19001017 * 1.) / db.ACS12_5yr_B11001001\n",
    "\n",
    "# ACS variables (alternatives)\n",
    "db1['PERCAP_ALT']   =  db.ACS12_5yr_B19025001 / (db.ACS12_5yr_B25008001 * 1.)  # HH income divided by persons in HHs\n",
    "db1['QESL_ALT']     =((db.ACS12_5yr_B06007005 + db.ACS12_5yr_B06007008) * 1.) /\\\n",
    "                       db.ACS12_5yr_B06007001                        # 5 and older who don't speak English very well\n",
    "db1['QED12LES_ALT'] = (db.ACS12_5yr_B16010002 * 1.) / db.ACS12_5yr_B16010001  # same value, simplified computation\n",
    "db1['QEXTRCT_ALT']  = (db.ACS12_5yr_C24050002 * 1.) / db.ACS12_5yr_C24050001  # same value, simplified computation\n",
    "db1['QSERV_ALT']    = (db.ACS12_5yr_C24050029 * 1.) / db.ACS12_5yr_C24050001  # same value, simplified computation\n",
    "db1['QNOAUTO_ALT']  = (db.ACS12_5yr_B08201002 * 1.) / db.ACS12_5yr_B08201001  # same value, simplified computation\n",
    "db1['MDGRENT_ALT']  =  db.ACS12_5yr_B25064001          # the original computed the median by hand so is not included\n",
    "db1['MHSEVAL_ALT']  =  db.ACS12_5yr_B25077001          # the original computed the median by hand so is not included\n",
    "db1['POPDENS']      =  db.ACS12_5yr_B01003001 / (db.SE_T02A_002 * 1.)  # I didn't understand QURBRURX\n",
    "\n",
    "# if no home value, assign the spatial lag of the estimate and SE\n",
    "homeval = db1['MHSEVAL_ALT'].copy()\n",
    "homeval_se = db.ACS12_5yr_B25077001s.copy()\n",
    "dbf = ps.open(os.path.join(spath,'USA_Counties_500k.dbf'))\n",
    "\n",
    "#Rename dbf GEOIDs to match homeval\n",
    "geoid=dbf.by_col('geoFIPS')\n",
    "\n",
    "shp_fips = pd.DataFrame(dbf.by_col('geoFIPS'), index=geoid)\n",
    "shp_fips = shp_fips.join(homeval)\n",
    "shp_fips = shp_fips.join(homeval_se)\n",
    "shp_fips['MHSEVAL_ALT_LAG'] = ps.lag_spatial(w, shp_fips.MHSEVAL_ALT)\n",
    "shp_fips['MHSEVAL_ALT_LAG_SE'] = ps.lag_spatial(w, shp_fips.ACS12_5yr_B25077001s)\n",
    "\n",
    "##Assign MHSEVAL_ALT and MHSEVAL_ALT_SE values back to neighborless counties \n",
    "#This might just act as a temporary fix... \n",
    "\n",
    "#Get original MHSEVAL_ALT values for problem counties as list\n",
    "mh=shp_fips.ix[shp_fips.MHSEVAL_ALT_LAG==0].MHSEVAL_ALT.tolist()\n",
    "\n",
    "#Reassign values to MHSEVAL_ALT_LAG\n",
    "shp_fips.ix[shp_fips.MHSEVAL_ALT_LAG==0,'MHSEVAL_ALT_LAG']=mh\n",
    "\n",
    "#Reassign missing standard error values\n",
    "mhs=shp_fips.ix[shp_fips.MHSEVAL_ALT_LAG_SE==0].ACS12_5yr_B25077001s.tolist()\n",
    "shp_fips.ix[shp_fips.MHSEVAL_ALT_LAG_SE==0,'MHSEVAL_ALT_LAG_SE']=mhs\n",
    "\n",
    "#Get rid of nan values - reassign MHSEVAL_ALT(_SE)\n",
    "shp_fips.MHSEVAL_ALT_LAG[np.isnan(shp_fips.MHSEVAL_ALT_LAG)] = shp_fips.MHSEVAL_ALT[np.isnan(shp_fips.MHSEVAL_ALT_LAG)] # replace NA with lag  \n",
    "shp_fips.MHSEVAL_ALT_LAG_SE[np.isnan(shp_fips.MHSEVAL_ALT_LAG_SE)] = shp_fips.ACS12_5yr_B25077001s[np.isnan(shp_fips.MHSEVAL_ALT_LAG_SE)] # replace NA with lag  \n",
    "\n",
    "\n",
    "db1['MHSEVAL_ALT_LAG'] = shp_fips['MHSEVAL_ALT_LAG']\n",
    "db1['MHSEVAL_ALT_LAG_SE'] = shp_fips['MHSEVAL_ALT_LAG_SE']\n",
    "db1.MHSEVAL_ALT[np.isnan(db1.MHSEVAL_ALT)] = db1.MHSEVAL_ALT_LAG[np.isnan(db1.MHSEVAL_ALT)]  \n",
    "# note: the lagged SE is pushed to the final column in the SE section below\n",
    "\n",
    "#############################\n",
    "\n",
    "# Decennial standard errors (using ACS data and alternative formulations)\n",
    "db1['MEDAGE_ACS_SE']  = db.ACS12_5yr_B01002001s\n",
    "\n",
    "db1['BLACK_ACS_SE']   = se_prop(db1.BLACK_ACS,           db.ACS12_5yr_B03002001,\n",
    "                                db.ACS12_5yr_B03002004s, db.ACS12_5yr_B03002001s)\n",
    "db1['QNATAM_ACS_SE']  = se_prop(db1.QNATAM_ACS,          db.ACS12_5yr_B03002001,\n",
    "                                db.ACS12_5yr_B03002005s, db.ACS12_5yr_B03002001s)\n",
    "db1['QASIAN_ACS_SE']  = se_prop(db1.QASIAN_ACS,          db.ACS12_5yr_B03002001,\n",
    "                                db.ACS12_5yr_B03002006s, db.ACS12_5yr_B03002001s)\n",
    "db1['QHISP_ACS_SE']   = se_prop(db1.QHISP_ACS,           db.ACS12_5yr_B03002001,\n",
    "                                db.ACS12_5yr_B03002012s, db.ACS12_5yr_B03002001s)\n",
    "\n",
    "QAGEDEP_ACS_sen       = se_sum(db.ACS12_5yr_B06001002s, db.ACS12_5yr_B09020001s)\n",
    "db1['QAGEDEP_ACS_SE'] = se_prop(db1.QAGEDEP_ACS, db.ACS12_5yr_B01003001,\n",
    "                                QAGEDEP_ACS_sen, db.ACS12_5yr_B01003001s)\n",
    "\n",
    "db1['QPUNIT_ACS_SE']  =se_ratio(db1.QPUNIT_ACS,          db.ACS12_5yr_B25002002,\n",
    "                                db.ACS12_5yr_B25008001s, db.ACS12_5yr_B25002002s)\n",
    "db1['PRENTER_ACS_SE'] = se_prop(db1.PRENTER_ACS,         db.ACS12_5yr_B25002001,\n",
    "                                db.ACS12_5yr_B25003003s, db.ACS12_5yr_B25002001s)\n",
    "db1['QNRRES_ACS_SE']  = se_prop(db1.QNRRES_ACS,          db.ACS12_5yr_B01003001,\n",
    "                                db.ACS12_5yr_B09020021s, db.ACS12_5yr_B01003001s)\n",
    "db1['QFEMALE_ACS_SE'] = se_prop(db1.QFEMALE_ACS,         db.ACS12_5yr_B01003001,\n",
    "                                db.ACS12_5yr_B01001026s, db.ACS12_5yr_B01003001s) \n",
    "db1['QFHH_ACS_SE']    = se_prop(db1.QFHH_ACS,            db.ACS12_5yr_B11001001,\n",
    "                                db.ACS12_5yr_B11001006s, db.ACS12_5yr_B11001001s)\n",
    "db1['QUNOCCHU_ACS_SE']= se_prop(db1.QUNOCCHU_ACS,        db.ACS12_5yr_B25002001,\n",
    "                                db.ACS12_5yr_B25002003s, db.ACS12_5yr_B25002001s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "\n",
    "# ACS standard errors (original)\n",
    "db1['PERCAP_SE']   = se_ratio(db1.PERCAP,              db.ACS12_5yr_B01003001,\n",
    "                              db.ACS12_5yr_B19025001s, db.ACS12_5yr_B01003001s)\n",
    "\n",
    "QESL_sen           = se_sum(db.ACS12_5yr_B16004029s, db.ACS12_5yr_B16004030s,\n",
    "                            db.ACS12_5yr_B16004034s, db.ACS12_5yr_B16004035s,\n",
    "                            db.ACS12_5yr_B16004039s, db.ACS12_5yr_B16004040s,\n",
    "                            db.ACS12_5yr_B16004044s, db.ACS12_5yr_B16004045s,\n",
    "                            db.ACS12_5yr_B16004051s, db.ACS12_5yr_B16004052s,\n",
    "                            db.ACS12_5yr_B16004056s, db.ACS12_5yr_B16004057s,\n",
    "                            db.ACS12_5yr_B16004061s, db.ACS12_5yr_B16004062s,\n",
    "                            db.ACS12_5yr_B16004066s, db.ACS12_5yr_B16004067s)\n",
    "QESL_sed           = se_sum(db.ACS12_5yr_B16004024s, db.ACS12_5yr_B16004046s,\n",
    "                            db.ACS12_5yr_B16004025s, db.ACS12_5yr_B16004047s)\n",
    "db1['QESL_SE']     = se_prop(db1.QESL, (db.ACS12_5yr_B16004024 + db.ACS12_5yr_B16004046) - \n",
    "                                       (db.ACS12_5yr_B16004025 + db.ACS12_5yr_B16004047),\n",
    "                             QESL_sen, QESL_sed)\n",
    "db1.QESL_SE = db1.QESL_SE.replace([np.inf,-np.inf], 0)\n",
    "db1.QESL_SE[db1.QESL==0] = 0\n",
    "\n",
    "QCVLUN_sen         = se_sum(db.ACS12_5yr_B23022025s, db.ACS12_5yr_B23022049s)\n",
    "db1['QCVLUN_SE']   = se_prop(db1.QCVLUN, db.ACS12_5yr_B23022001, \n",
    "                             QCVLUN_sen, db.ACS12_5yr_B23022001s)\n",
    "\n",
    "db1['QPOVTY_SE']   = se_prop(db1.QPOVTY,              db.ACS12_5yr_B17021001,\n",
    "                             db.ACS12_5yr_B17021002s, db.ACS12_5yr_B17021001s)\n",
    "db1['QMOHO_SE']    = se_prop(db1.QMOHO,               db.ACS12_5yr_B25024001,\n",
    "                             db.ACS12_5yr_B25024010s, db.ACS12_5yr_B25024001s)\n",
    "\n",
    "QED12LES_sen       = se_sum(db.ACS12_5yr_B15002003s, db.ACS12_5yr_B15002004s,\n",
    "                            db.ACS12_5yr_B15002005s, db.ACS12_5yr_B15002006s,\n",
    "                            db.ACS12_5yr_B15002007s, db.ACS12_5yr_B15002008s,\n",
    "                            db.ACS12_5yr_B15002009s, db.ACS12_5yr_B15002010s,\n",
    "                            db.ACS12_5yr_B15002020s, db.ACS12_5yr_B15002021s,\n",
    "                            db.ACS12_5yr_B15002022s, db.ACS12_5yr_B15002023s,\n",
    "                            db.ACS12_5yr_B15002024s, db.ACS12_5yr_B15002025s,\n",
    "                            db.ACS12_5yr_B15002026s, db.ACS12_5yr_B15002027s)\n",
    "db1['QED12LES_SE'] = se_prop(db1.QED12LES, db.ACS12_5yr_B15002001,\n",
    "                             QED12LES_sen, db.ACS12_5yr_B15002001s)\n",
    "\n",
    "db1['QFEMLBR_SE']  = se_prop(db1.QFEMLBR,             db.ACS12_5yr_C24010001,\n",
    "                             db.ACS12_5yr_C24010038s, db.ACS12_5yr_C24010001s)\n",
    "\n",
    "\n",
    "QEXTRCT_sen        = se_sum(db.ACS12_5yr_C24030003s, db.ACS12_5yr_C24030030s)\n",
    "db1['QEXTRCT_SE']  = se_prop(db1.QEXTRCT, db.ACS12_5yr_C24030001,\n",
    "                             QEXTRCT_sen, db.ACS12_5yr_C24030001s)\n",
    "\n",
    "QSERV_sen          = se_sum(db.ACS12_5yr_C24010019s, db.ACS12_5yr_C24010055s)\n",
    "db1['QSERV_SE']    = se_prop(db1.QSERV, db.ACS12_5yr_C24010001,\n",
    "                             QSERV_sen, db.ACS12_5yr_C24010001s)\n",
    "\n",
    "db1['QSSBEN_SE']   = se_prop(db1.QSSBEN,              db.ACS12_5yr_B19055001, \n",
    "                             db.ACS12_5yr_B19055002s, db.ACS12_5yr_B19055001s)\n",
    "\n",
    "QNOAUTO_sen        = se_sum(db.ACS12_5yr_B25044003s, db.ACS12_5yr_B25044010s)\n",
    "db1['QNOAUTO_SE']  = se_prop(db1.QNOAUTO, db.ACS12_5yr_B25044001,\n",
    "                             QNOAUTO_sen, db.ACS12_5yr_B25044001s)\n",
    "\n",
    "db1['QFAM_SE']     = se_prop(db1.QFAM,                db.ACS12_5yr_B09002001, \n",
    "                             db.ACS12_5yr_B09002002s, db.ACS12_5yr_B09002001s)\n",
    "db1.QFAM_SE = db1.QFAM_SE.replace([np.inf,-np.inf], 0)\n",
    "\n",
    "db1['QRICH200K_SE']= se_prop(db1.QRICH200K,           db.ACS12_5yr_B11001001, \n",
    "                             db.ACS12_5yr_B19001017s, db.ACS12_5yr_B11001001s)\n",
    "\n",
    "\n",
    "#############################\n",
    "\n",
    "# ACS standard errors (alternatives)\n",
    "db1['PERCAP_ALT_SE']   =  se_ratio(db1.PERCAP_ALT,          db.ACS12_5yr_B25008001,\n",
    "                                   db.ACS12_5yr_B19025001s, db.ACS12_5yr_B25008001s)\n",
    "\n",
    "QESL_ALT_sen           = se_sum(db.ACS12_5yr_B06007005s, db.ACS12_5yr_B06007008s)\n",
    "db1['QESL_ALT_SE']     = se_prop(db1.QESL_ALT, db.ACS12_5yr_B06007001,\n",
    "                                 QESL_ALT_sen, db.ACS12_5yr_B06007001s)\n",
    "\n",
    "db1['QED12LES_ALT_SE'] = se_prop(db1.QED12LES_ALT,        db.ACS12_5yr_B16010001,\n",
    "                                 db.ACS12_5yr_B16010002s, db.ACS12_5yr_B16010001s)\n",
    "db1['QEXTRCT_ALT_SE']  = se_prop(db1.QEXTRCT_ALT,         db.ACS12_5yr_C24050001,\n",
    "                                 db.ACS12_5yr_C24050002s, db.ACS12_5yr_C24050001s)\n",
    "db1['QSERV_ALT_SE']    = se_prop(db1.QSERV_ALT,           db.ACS12_5yr_C24050001,\n",
    "                                 db.ACS12_5yr_C24050029s, db.ACS12_5yr_C24050001s)\n",
    "db1['QNOAUTO_ALT_SE']  = se_prop(db1.QNOAUTO_ALT,         db.ACS12_5yr_B08201001,\n",
    "                                 db.ACS12_5yr_B08201002s, db.ACS12_5yr_B08201001s)\n",
    "db1['MDGRENT_ALT_SE']  =  db.ACS12_5yr_B25064001s\n",
    "db1['MHSEVAL_ALT_SE']  =  db.ACS12_5yr_B25077001s\n",
    "db1.MHSEVAL_ALT_SE[np.isnan(db1.MHSEVAL_ALT)] = db1.MHSEVAL_ALT_LAG_SE[np.isnan(db1.MHSEVAL_ALT)] # replace NA with lag  \n",
    "db1.MHSEVAL_ALT_SE[np.isnan(db1.MHSEVAL_ALT_SE)] = db1.MHSEVAL_ALT_LAG_SE[np.isnan(db1.MHSEVAL_ALT_SE)] # replace NA with lag  \n",
    "db1['POPDENS_SE']      =se_ratio(db1.POPDENS,             db.SE_T02A_002,\n",
    "                                 db.ACS12_5yr_B01003001s, 0)  # these are nearly all zero since county pops tend to have 0 MOE\n",
    "\n",
    "#############################\n",
    "\n",
    "# Tests to validate equivalency \n",
    "def equal_test(orig, alt):\n",
    "    if np.equal(orig, alt).sum() != db.shape[0]:\n",
    "        if (db.shape[0] - np.equal(orig, alt).sum()) == np.isnan(orig).sum() == np.isnan(alt).sum():\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception, 'problem'\n",
    "equal_test(db1.QUNOCCHU, db1.QUNOCCHU_ALT)\n",
    "equal_test(db1.QED12LES, db1.QED12LES_ALT)\n",
    "equal_test(db1.QEXTRCT,  db1.QEXTRCT_ALT)\n",
    "equal_test(db1.QSERV,    db1.QSERV_ALT)        \n",
    "equal_test(db1.QNOAUTO,  db1.QNOAUTO_ALT)    \n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "\n",
    "# Add in the sample sizes\n",
    "db1['sample_person']  =  db.ACS12_5yr_B00001001\n",
    "db1['sample_hu']  =  db.ACS12_5yr_B00002001\n",
    "\n",
    "#############################\n",
    "\n",
    "#the final data frame is written to disk\n",
    "db1.to_csv(os.path.join(path,'sovi_inputs.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Analysis\n",
    "\n",
    "Here we load the data `db1` or alternative add a refernce to db1 with a new name `US_ALL`. We describe each of the input variables with a human readable name and outline their expected contributions to vulnerability.  For example, the variable `MEDAGE_ACS`, which measures the median age in the county is expected to have a positive (\"pos\") contribution to social vulnerability - the logic is that older populations are more vulnerable to disasters.   Similarly the variable `QRICH200K`, which measures the portion of hosuing units making over $200k/year is expected to have a negative (\"neg\") contribution to social vulnerability.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import Data\n",
    "US_All = db1.copy()\n",
    "US_All['Geo_FIPS'] = US_All.index.values\n",
    "#US_All = pd.read_csv(path+'/data/input/sovi_inputs.csv')\n",
    "#US_All.index = US_All.Geo_FIPS\n",
    "\n",
    "\n",
    "# attribute name and expected influence on vulnerability\n",
    "input_names = [['MEDAGE_ACS','pos','person'],\n",
    "               ['BLACK_ACS','pos','person'],   \n",
    "               ['QNATAM_ACS','pos','person'],  \n",
    "               ['QASIAN_ACS','pos','person'],  \n",
    "               ['QHISP_ACS','pos','person'],   \n",
    "               ['QAGEDEP_ACS','pos','person'], \n",
    "               ['QPUNIT_ACS','pos','person'],  \n",
    "               ['PRENTER_ACS','pos','hu'], \n",
    "               ['QNRRES_ACS','pos','person'],  \n",
    "               ['QFEMALE_ACS','pos','person'], \n",
    "               ['QFHH_ACS','pos','hu'],    \n",
    "               ['QUNOCCHU_ACS','pos','hu'],\n",
    "               ['PERCAP_ALT','neg','person'],  \n",
    "               ['QESL_ALT','pos','person'],    \n",
    "               ['QCVLUN','pos','person'], \n",
    "               ['QPOVTY','pos','person'],\n",
    "               ['QMOHO','pos','hu'],\n",
    "               ['QED12LES_ALT','pos','person'],\n",
    "               ['QFEMLBR','pos','person'], \n",
    "               ['QEXTRCT_ALT','pos','person'], \n",
    "               ['QSERV_ALT','pos','person'],   \n",
    "               ['QSSBEN','pos','hu'],\n",
    "               ['QNOAUTO_ALT','pos','hu'], \n",
    "               ['QFAM','neg','person'], \n",
    "               ['QRICH200K','neg','hu'],\n",
    "               ['MDGRENT_ALT','neg','hu'], \n",
    "               ['MHSEVAL_ALT','neg','hu'], \n",
    "               ['POPDENS','pos','person']] \n",
    "\n",
    "#Get attribute names\n",
    "attr_names=[j[0] for j in input_names]\n",
    "#cols = [c for c in US_All.columns if c.find('_SE') == -1]\n",
    "\n",
    "attr_names.append('Geo_FIPS')\n",
    "#US_All = US_All.dropna(axis=0) #two counties misisng data in state 15 and 48\n",
    "US_All = US_All[attr_names]\n",
    "US_All['stateID'] = US_All.Geo_FIPS.str.slice(0,3,1)\n",
    "attr_names.remove('Geo_FIPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Flipping Signs\n",
    "\n",
    "To ensure that each variable contributes as expected to the final Sovi Index following Eric Tate (2012?) we flip the signs of the input data.  Variables where a high value are expected to contribute negatively to Social vulnerability have their signs flipped, such that positive values become negative.  This has the effect of reversing the sign of the scores used to compute the index.  Consider the variable measuring the percent of households making over \\$200,000 per year, if the mean at the county-level in a state was 5\\% a county two SD above the mean, say one where 10\\% of the population made mroe than $200K, that would have a positive z-score (+2).  However, this high prevalance of wealthy people should reduce the vulnerability, by flipping the signs we ensure that that mean is now -5\\% and a value of -10\\% is two standard deviations *below* the mean.  Thus when multiplied by its (positive) loading a county whit high wealth will have a lower social vulnerability index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input data prep\n",
    "# --swap signs of the attributes expected to have a \"negative\" affect on vulnerability\n",
    "for name, sign, sample in input_names:\n",
    "    if sign == 'neg':\n",
    "        US_All[name] = -US_All[name].values\n",
    "    elif sign == 'pos':\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception, \"problem\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset FEMA Regions and Calculate SOVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "##SOVI FOR FEMA REGIONS\n",
    "#########################################\n",
    "#Build FEMA subRegions Dict values= state ID's\n",
    "FEMA_subs= {'FEMA_1':['g23','g50','g33','g25','g09','g44']}\n",
    "FEMA_subs['FEMA_2'] = ['g36','g34']\n",
    "FEMA_subs['FEMA_3'] = ['g42','g10','g11','g24','g51','g54']\n",
    "FEMA_subs['FEMA_4'] = ['g21','g47','g37','g28','g01','g13','g45','g12']\n",
    "FEMA_subs['FEMA_5'] = ['g27','g55','g26','g17','g18','g39']\n",
    "FEMA_subs['FEMA_6'] = ['g35','g48','g40','g05','g22']\n",
    "FEMA_subs['FEMA_7'] = ['g31','g19','g20','g29']\n",
    "FEMA_subs['FEMA_8'] = ['g30','g38','g56','g46','g49','g08']\n",
    "FEMA_subs['FEMA_9'] = ['g06','g32','g04']\n",
    "FEMA_subs['FEMA_10'] = ['g53','g41','g16']\n",
    "\n",
    "#Dict to hold variable loadings\n",
    "varContrib = {}\n",
    "\n",
    "#Multiindexed DataFrame to hold all FEMA SOVI Scores\n",
    "geoLevels = US_All.Geo_FIPS\n",
    "femaLevels = FEMA_subs.keys()\n",
    "geoLabels = []\n",
    "femaLabels = []\n",
    "for f in femaLevels:\n",
    "    femaRegionIndexes = US_All[US_All['stateID'].isin(FEMA_subs[f])].index.values\n",
    "    geoLabels.extend([US_All.index.get_loc(i) for i in femaRegionIndexes])\n",
    "    femaLabels.extend(np.repeat(femaLevels.index(f), len(femaRegionIndexes)))\n",
    "\n",
    "US_femaSub_Multi_Index = pd.MultiIndex(levels=[femaLevels, geoLevels], \n",
    "                                    labels=[femaLabels, geoLabels], \n",
    "                                    names=['FEMA_Region', 'Geo_FIPS'])\n",
    "\n",
    "FEMA_Region_Sovi_Score = pd.DataFrame(index=US_femaSub_Multi_Index, columns=['sovi', 'rank']) \n",
    "\n",
    "for i in FEMA_subs:\n",
    "    \n",
    "    #Subset FEMA subregion\n",
    "    FEMARegionData=US_All[US_All['stateID'].isin(FEMA_subs[i])]\n",
    "\n",
    "    # compute SoVI\n",
    "    inputData = FEMARegionData.drop(['Geo_FIPS','stateID'], axis = 1, inplace = False)\n",
    "    pca = SPSS_PCA(inputData, reduce=True, varimax=True)\n",
    "    sovi_actual = pca.scores_rot.sum(1)\n",
    "    sovi_actual = pd.DataFrame(sovi_actual, index=FEMARegionData.Geo_FIPS, columns=['sovi'])\n",
    "    attrib_contribution = pca.weights_rot.sum(1)\n",
    "    \n",
    "    FEMA_Region_Sovi_Score.loc[i, 'sovi'] = sovi_actual.values\n",
    "    #ADD RANKabs(sovi_actual).apply(rankdata, axis=0, method='average')\n",
    "        \n",
    "    ##Write attribute contribution output     \n",
    "    #Generate dictionary for all net loadings by variable and region\n",
    "    varContrib[i]=zip(attr_names,attrib_contribution.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute National SoVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "##Compute National SoVI\n",
    "#######################\n",
    "# compute SoVI\n",
    "inputData = US_All.drop(['Geo_FIPS','stateID'], axis = 1, inplace = False)\n",
    "pca = SPSS_PCA(inputData, reduce=True, varimax=True)\n",
    "sovi_actual = pca.scores_rot.sum(1)\n",
    "sovi_actual = pd.DataFrame(sovi_actual, index=US_All.Geo_FIPS, columns=['sovi'])\n",
    "attrib_contribution = pca.weights_rot.sum(1)\n",
    "US_All_Full_Sovi_Rank = abs(sovi_actual).apply(rankdata, axis=0, method='average')\n",
    "     \n",
    "#Generate dictionary for all net loadings by variable and region\n",
    "varContrib['USA']=zip(attr_names,attrib_contribution.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute SoVI for elected state(s) in each FEMA Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "##State Analysis     \n",
    "#############################################\n",
    "#Create New England conglomerate of states\n",
    "US_All.loc[US_All.stateID.isin(['g23','g33','g25']), 'stateID'] = 'g23g33g25'\n",
    "\n",
    "stateList = ['g23g33g25', 'g36','g51','g13','g17','g48','g29','g46','g06','g16']\n",
    "\n",
    "for st in stateList:\n",
    "    #Subset FEMA subregion\n",
    "    stateData=US_All[US_All.stateID == st]\n",
    "\n",
    "    # compute SoVI\n",
    "    inputData = stateData.drop(['Geo_FIPS','stateID'], axis = 1, inplace = False)\n",
    "    pca = SPSS_PCA(inputData, reduce=True, varimax=True)\n",
    "    sovi_actual = pca.scores_rot.sum(1)\n",
    "    sovi_actual = pd.DataFrame(sovi_actual, index=stateData.Geo_FIPS, columns=['sovi'])\n",
    "    attrib_contribution = pca.weights_rot.sum(1)\n",
    "    \n",
    "    #sovi_alt_computation = (zinputs * attrib_contribution).sum(1) # this is just a check\n",
    "    #sovi_alt_computation = pd.DataFrame(sovi_alt_computation, columns=['sovi'])\n",
    "    #if not np.allclose(sovi_actual, sovi_alt_computation):\n",
    "    #   raise Exception, \"mismatch\"\n",
    "        \n",
    "    ##Write attribute contribution output     \n",
    "    #Generate dictionary for all net loadings by variable and region\n",
    "    varContrib[st]=zip(attr_names,attrib_contribution.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "##Consolidate variable net contribs and ranks \n",
    "#############################################\n",
    "netContribCols = varContrib.keys()\n",
    "\n",
    "netContrib = pd.DataFrame(columns=netContribCols, index=attr_names)\n",
    "\n",
    "for r in varContrib.keys():\n",
    "    for name, value in varContrib[r]:\n",
    "        netContrib.loc[name][r] = value\n",
    "\n",
    "#variable rank using absolute value      \n",
    "rankContrib = abs(netContrib).apply(rankdata, axis=0, method='average')\n",
    "rankContrib = (28-rankContrib) + 1\n",
    "\n",
    "\n",
    "combContrib = pd.DataFrame(columns=netContribCols, index=attr_names)\n",
    "#can't think of a more elegant way to do this\n",
    "for aRow in range(netContrib.shape[1]):\n",
    "    for aCol in range(netContrib.shape[0]):\n",
    "        combContrib.ix[aCol][aRow] = str(round(netContrib.ix[aCol][aRow], 2)) + ' (' + str(int(rankContrib.ix[aCol][aRow])) + ')'\n",
    "\n",
    "#reorder table        \n",
    "cols = ['USA', 'FEMA_1', 'g23g33g25', \n",
    "'FEMA_2', 'g36','FEMA_3', 'g51', 'FEMA_4', 'g13', 'FEMA_5', 'g17',\n",
    "'FEMA_6', 'g48', 'FEMA_7', 'g29', 'FEMA_8', 'g46', 'FEMA_9', 'g06', 'FEMA_10', \n",
    "'g16']\n",
    "combContrib = combContrib[cols]\n",
    "\n",
    "#human readable variable names\n",
    "desc = ['Median Age',\n",
    "'Pop African-American (%)',\n",
    "'Pop Native American (%)',\n",
    "'Pop Asian (%)',\n",
    "'Pop Hispanic (%)',\n",
    "'Age Dependency (%)',\n",
    "'Persons Per Housing Unit',\n",
    "'Rental Housing (%)',\n",
    "'Nursing Home Residents (%)',\n",
    "'Pop Female (%)',\n",
    "'Female-Headed Households (%)',\n",
    "'Vacant Housing (%)',\n",
    "'Per-Capita Income',\n",
    "'English as Second Language (%)',\n",
    "'Unemployment (%)',\n",
    "'Poverty (%)',\n",
    "'Mobile Homes (%)',\n",
    "'Adults Completed <Grade 12 (%)',\n",
    "'Female Employment (%)',\n",
    "'Extractive Sector Employment (%)',\n",
    "'Service Sector Employment (%)',\n",
    "'Social Security Income (%)',\n",
    "'No Automobile (%)',\n",
    "'Children in Married Families (%)',\n",
    "'Annual Income >$200K (%)',\n",
    "'Median Rent',\n",
    "'Median Home Value',\n",
    "'Population Density']\n",
    "\n",
    "#set descriptive names\n",
    "combContrib.index = desc\n",
    "\n",
    "#Sort variable list based on importance rank.\n",
    "#USvarRanks = rankContrib.USA.copy() #have to make a copy to sort index\n",
    "#USvarRanks.sort('USA')\n",
    "#dropLevels = USvarRanks.index\n",
    "\n",
    "rankContrib.USA.apply(np.argsort)\n",
    "\n",
    "\n",
    "#write out results\n",
    "combContrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################\n",
    "#Drop 1 Analysis\n",
    "#################\n",
    "\n",
    "\n",
    "#Sort variable list based on importance rank.\n",
    "USvarRanks = rankContrib.USA.copy() #have to make a copy to sort index\n",
    "USvarRanks.sort('USA')\n",
    "dropLevels = USvarRanks.index\n",
    "\n",
    "#build multindex\n",
    "geoLevels = US_All.Geo_FIPS\n",
    "geoLabels = []\n",
    "for _ in range(len(dropLevels)):\n",
    "    geoLabels.extend(range(len(geoLevels)))\n",
    "dropLabels = np.repeat(range(len(dropLevels)), len(geoLevels))\n",
    "\n",
    "US_Drop1_Multi_Index = pd.MultiIndex(levels=[dropLevels, geoLevels], \n",
    "                                    labels=[dropLabels, geoLabels], \n",
    "                                    names=['DroppedVar', 'Geo_FIPS'])\n",
    "                                    \n",
    "US_Drop1_NetContrib = pd.DataFrame(index=dropLevels, columns=dropLevels)                     \n",
    "\n",
    "US_SoVI_Drop1_Score = pd.DataFrame(index=US_Drop1_Multi_Index, columns=['sovi']) \n",
    "\n",
    "\n",
    "#Compute drop-one \n",
    "for j in dropLevels:\n",
    "    US_dropj = US_All.drop([j,'Geo_FIPS', 'stateID'], axis = 1, inplace = False)\n",
    "    pca = SPSS_PCA(US_dropj, reduce=True, varimax=True)\n",
    "    sovi_actual = pca.scores_rot.sum(1)\n",
    "    sovi_actual = pd.DataFrame(sovi_actual, index=geoLevels, columns=['sovi'])\n",
    "    US_SoVI_Drop1_Score.loc[j, 'sovi'] = sovi_actual.values\n",
    "    attrib_contribution = pd.DataFrame(data=pca.weights_rot.sum(1), index=US_dropj.columns)\n",
    "    #print(j +\" \" + str(np.isnan(attrib_contribution.values).sum()))\n",
    "    attrib_contribution = attrib_contribution.transpose()\n",
    "    attrib_contribution.index = [j]\n",
    "    #print(attrib_contribution.loc[j,:])\n",
    "    US_Drop1_NetContrib.loc[j, attrib_contribution.columns] = attrib_contribution.loc[j,:] #.values\n",
    "\n",
    "#Sort descriptive labels\n",
    "USvarRanks = rankContrib.USA.copy()\n",
    "USvarRanks.index = desc\n",
    "USvarRanks.sort('USA')\n",
    "US_Drop1_NetContrib.index = USvarRanks.index\n",
    "US_Drop1_NetContrib.columns = USvarRanks.index\n",
    "    \n",
    "US_Drop1_NetContrib = US_Drop1_NetContrib.T #T so columns indexes dropped variable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "US_Drop1_NetContrib=US_Drop1_NetContrib.convert_objects(convert_numeric=True)\n",
    "US_Drop1_NetContrib = US_Drop1_NetContrib.apply(lambda x: np.round(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'US_Drop1_NetContrib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ac4e7fb02d81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#plt.figure(figsize=(20, 16))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUS_Drop1_NetContrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUS_Drop1_NetContrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_kws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'US_Drop1_NetContrib' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "#Reorder and apply variable description to labels\n",
    "#USvarRanks = rankContrib.USA.copy() #have to make a copy to sort index\n",
    "#USvar\n",
    "#dropLevels = USvarRanks.indexdesc\n",
    "\n",
    "#plt.figure(figsize=(20, 16))\n",
    "mask=np.isnan(US_Drop1_NetContrib)\n",
    "sns.heatmap(US_Drop1_NetContrib, annot=True, linewidths=.25, vmin=-1, vmax=1, annot_kws={\"size\": 7})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###Compute Correlation\n",
    "\n",
    "In this section we compute spearman correlation for differe subsets of counties.  This is our check of the concurrent validity of of the metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "##Compute spearman correlation\n",
    "######################\n",
    "#compute US base ranks for all drop 1 correlations\n",
    "spearmanr(US_SoVI_Drop1_Score[])\n",
    "\n",
    "#rankContrib = (28-rankContrib) + 1\n",
    "#compare Fema regions and US\n",
    "\n",
    "#compare fema regions and states\n",
    "\n",
    "#compare states and US\n",
    "\n",
    "#compare drop 1 to full sovi\n",
    "scipy.stats.mstats.spearmanr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
